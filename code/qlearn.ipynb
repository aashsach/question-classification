{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# modify_accordingly\n",
    "\n",
    "#should contain \"train_questions.txt\" and \"test_questions.txt and \"train_labels.csv\"\n",
    "data_dir  = \"./data/\" \n",
    "\n",
    "#produces test_labels.csv and best_model.pkl\n",
    "save_dir = \"./output/\"\n",
    "\n",
    "#dir containg files of synpnyms\n",
    "synonym_dir = \"./resources/synonyms/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"loading necessary resources and packages. Make sure you have them\"\n",
    "import os,sys\n",
    "import cPickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class_labesl = {0 : \"Abbreviation\",\n",
    "                1 : \"Human\",\n",
    "                2 : \"Location\",\n",
    "                3 : \"Description\",\n",
    "                4 : \"Entity\",\n",
    "                5 : \"Numeric\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#make sure all these files are present!\n",
    "\n",
    "syn_list =  ['money', 'place', 'comp', 'word', 'letter', 'city', 'an', 'code', 'unit', 'def', 'title',\n",
    " 'symbol', 'singleBe', 'popu', 'food', 'country', 'InOn', 'How', 'tech', 'speak', 'At',\n",
    " 'last', 'state', 'presentBe', 'On', 'lang', 'pastBe', 'sport', 'quot', 'ord', 'desc',\n",
    " 'speed', 'cause', 'eff', 'body', 'be', 'num', 'event', 'peop', 'plant', 'weight',\n",
    " 'In', 'animal', 'art', 'accompany', 'mount', 'culture', 'substance', 'abbreviation',\n",
    " 'color', 'prof', 'currency', 'date', 'other', 'fast', 'instrument', 'prod', 'temp',\n",
    " 'religion', 'loca', 'dist', 'dimen', 'perc', 'Why', 'big', 'job', 'name', 'group', 'dise',\n",
    " 'univ', 'vessel', 'do', 'Who', 'What', 'time', 'Where', 'stand', 'term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "synonyms = {}\n",
    "for l in syn_list:\n",
    "    with open(synonym_dir+l) as f:\n",
    "        for word in f.read().split():            \n",
    "            synonyms[stemmer.stem(word)] = l.upper()\n",
    "\n",
    "syn_dict = {\"PROF\" : wn.synsets(\"person\",[\"n\"])[0],\n",
    "            \"LOCA\" : wn.synsets(\"place\",[\"n\"])[0],\n",
    "            \"SUBSTANCE\":wn.synsets(\"substance\",[\"n\"])[0],\n",
    "            \"NUMBER\":wn.synsets(\"quantity\",[\"n\"])[0]\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#diffeerent feature dictionaries\n",
    "\n",
    "#train_data\n",
    "data_dict = {} #text\n",
    "data_tok_dict = {} #tokenised_text\n",
    "data_ne_dict = {} #named_entity_tagged\n",
    "data_pos_dict = {} #pos\n",
    "data_tag_dict = {} #pos_rag\n",
    "data_nv_dict = {} #noun_vector\n",
    "\n",
    "#test_data\n",
    "test_dict = {\"data\":[],\"tok\":[],\"ne\":[],\"pos\":[],\"tag\":[],\"nv\":[], }\n",
    "\n",
    "#test_id for output\n",
    "test_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load_train_data\n",
    "\n",
    "data_key ={}\n",
    "\n",
    "with open(\"./data/train_questions.txt\") as f:\n",
    "    f.readline();\n",
    "    print \"Loading... train_data\"\n",
    "    for l in f.readlines():\n",
    "        key, ques = l.split(\",\")\n",
    "        data_key[int(key.strip())] = ques.strip()\n",
    "        \n",
    "with open(data_dir+\"train_labels.csv\") as f:\n",
    "    f.readline();\n",
    "    print \"Loading... train_data\"\n",
    "    for l in f.readlines():\n",
    "        key, label = l.split(\",\")\n",
    "        data_dict.setdefault(int(label.strip()),[]).append(data_key[int(key.strip())])\n",
    "        \n",
    "del data_key       \n",
    "\n",
    "#load_test_data\n",
    "with open(data_dir+\"test_questions.txt\") as f:\n",
    "    f.readline();\n",
    "    print \"Loading... test_data\"\n",
    "    for l in f.readlines():\n",
    "        key,ques = l.split(\",\")\n",
    "        test_id.append(int(key.strip()))\n",
    "        test_dict[\"data\"].append(ques.strip())\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_data(sentence):\n",
    "    tok = []\n",
    "    ne = []\n",
    "    pos = []\n",
    "    tag = []\n",
    "    nv = []\n",
    "    \n",
    "    ne_list = {}\n",
    "    \n",
    "    sentence = unicode(sentence,\"utf-8\")\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    for n in doc.ents:\n",
    "         ne_list[n.start] =  (n.end,n.label_)\n",
    "            \n",
    "    for n in doc:\n",
    "        tok.append(n.text)\n",
    "        pos.append(n.pos_)\n",
    "        tag.append(n.tag_)\n",
    "    \n",
    "    cur_stop = 0\n",
    "    for i,t in enumerate(tok):\n",
    "        if i < cur_stop:\n",
    "            continue\n",
    "        if i in ne_list:\n",
    "            cur_stop, label = ne_list[i]\n",
    "            ne.append(label)\n",
    "            \n",
    "            if label in [\"LOC\",\"GPE\",\"FACILITY\"]:\n",
    "                nv.append(\"loca\")\n",
    "            elif label == \"NPOR\":\n",
    "                nv.append(\"lan\")\n",
    "            elif label == \"PERSON\":\n",
    "                nv.append(\"prof\")\n",
    "            else:\n",
    "                if stemmer.stem(label) in synonyms:\n",
    "                    nv.append(synonyms[stemmer.stem(label)])\n",
    "                else:\n",
    "                    nv.append(stemmer.stem(label))\n",
    "            \n",
    "        else:\n",
    "            ne.append(t)\n",
    "            \n",
    "            if i==0:\n",
    "                nv.append(t)\n",
    "                continue\n",
    "            if stemmer.stem(t) in synonyms:\n",
    "                nv.append(synonyms[stemmer.stem(t)])\n",
    "                continue\n",
    "            if pos[i] ==\"NOUN\":\n",
    "                try:\n",
    "                    syn = wn.synsets(n.lemma_,[\"n\"])[0]\n",
    "                    min_dist = float(\"inf\")\n",
    "                    label = None\n",
    "                    for key,val in syn_dict.items():\n",
    "                        sim = syn.shortest_path_distance(val)\n",
    "                        if  sim and sim < min_dist:\n",
    "                            min_dist = sim\n",
    "                            label = key\n",
    "                    if min_dist<=6:\n",
    "                        nv.append(stemmer.stem(label))\n",
    "                        continue\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                if stemmer.stem(t) in synonyms:\n",
    "                    nv.append(synonyms[stemmer.stem(t)])\n",
    "                else:\n",
    "                    nv.append(stemmer.stem(t))\n",
    "    \n",
    "    assert len(tok) >= len(ne)\n",
    "    return tok , ne , pos ,tag ,nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"Extracting Features...\"\n",
    "for l,ques in data_dict.items():\n",
    "    for q in ques:\n",
    "        \n",
    "        tok,ne,pos,tag,nv = parse_data(q)\n",
    "        \n",
    "        data_tok_dict.setdefault(l,[]).append(tok)\n",
    "        data_ne_dict.setdefault(l,[]).append(ne)\n",
    "        data_pos_dict.setdefault(l,[]).append(pos)\n",
    "        data_tag_dict.setdefault(l,[]).append(tag)\n",
    "        data_nv_dict.setdefault(l,[]).append(nv)\n",
    "        \n",
    "for q in test_dict[\"data\"]:\n",
    "    \n",
    "    tok,ne,pos,tag,nv = parse_data(q)\n",
    "\n",
    "    test_dict[\"tok\"].append(tok)\n",
    "    test_dict[\"ne\"].append(ne)\n",
    "    test_dict[\"pos\"].append(pos)\n",
    "    test_dict[\"tag\"].append(tag)\n",
    "    test_dict[\"nv\"].append(nv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_size = len(test_dict[\"data\"])\n",
    "test_y = dict(zip(range(test_size), [None]*test_size))\n",
    "cnt = 0\n",
    "print \"Running Rule Based Classifier...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#abbrev rules\n",
    "for idx,label in test_y.items():\n",
    "    if label is not None:\n",
    "        continue\n",
    "    d = test_dict[\"data\"][idx]\n",
    "    regex1 = r\"\\b[a-zA-Z&\\.]{1,}[A-Z]\\b\\.?\"\n",
    "    regex2 = r\"\\b[A-Z\\.]{2,}[a-z]\\b\\.?\"\n",
    "    regex = r\"What is [a-zA-Z&\\.]*[A-Z]\\b\\.?\"\n",
    "    r = re.findall(regex1,d) +re.findall(regex2,d)\n",
    "    if (len(r) and (\"mean\" in d.lower() or re.match(regex,d))) or \"stand for\" in d.lower() or \"abbreviation\" in d.lower() or \"acronym\" in d.lower():\n",
    "        cnt+=1\n",
    "        test_y[idx] = 0\n",
    "        continue\n",
    "\n",
    "### who whom whose rules\n",
    "    ques = test_dict[\"tok\"][idx]\n",
    "    if ques[0] in [\"Whom\",\"Whose\",\"Who\"]:\n",
    "        cnt+=1\n",
    "        test_y[idx] = 1\n",
    "        continue\n",
    "    if ques[0] in [\"Why\",\"If\"]:\n",
    "        cnt+=1\n",
    "        test_y[idx] = 3\n",
    "        continue\n",
    "        \n",
    "\n",
    "\n",
    "### location rules\n",
    "\n",
    "    ques = test_dict[\"tok\"][idx]\n",
    "    if ques[0]==\"Where\":\n",
    "        cnt+=1\n",
    "        test_y[idx] = 2\n",
    "        continue\n",
    "    if ques[0]==\"Is\" and \"capital\" in ques and \"GPE\" in test_dict[\"ne\"][idx]:\n",
    "        cnt+=1\n",
    "        test_y[idx] = 2\n",
    "        continue\n",
    "    if ques[0] == \"Which\" and \"place\" in ques:\n",
    "        cnt+=1\n",
    "        test_y[idx] = 2\n",
    "        continue\n",
    "\n",
    "\n",
    "### how define describe explain rules\n",
    "\n",
    "    ques = test_dict[\"tok\"][idx]\n",
    "    if ques[0] in [\"Define\",\"Describe\",\"Explain\",\"Suggest\"] or (ques[0]==\"Give\" and ques[1]==\"reason\"):\n",
    "        cnt+=1\n",
    "        test_y[idx] = 3\n",
    "        continue\n",
    "    if ques[0]==\"How\":\n",
    "        if test_dict[\"tag\"][idx][1] in [\"JJ\",\"RB\"]:\n",
    "            cnt+=1\n",
    "            test_y[idx] = 5\n",
    "            continue\n",
    "        cnt+=1\n",
    "        test_y[idx] = 3\n",
    "        continue\n",
    "already = [a for a,b in test_y.items() if b is not None]\n",
    "assert cnt == len(already)\n",
    "print cnt,\"questions classified using Rule Based Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_svm_features():\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        train_x+=[\" \".join(a+b) for a,b in zip(data_nv_dict[i], data_tok_dict[i])]\n",
    "        train_y+=[i]*len(data_dict[i])\n",
    "        \n",
    "    assert len(train_x)==len(train_y)\n",
    "\n",
    "    test_x = [\" \".join(a+b) for a,b in zip(test_dict[\"nv\"], test_dict[\"tok\"])]    \n",
    "    \n",
    "    return train_x, train_y, test_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"preparing data for SVM\"\n",
    "train_x, train_y, test_x = get_svm_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(np.hstack((train_x,test_x)));\n",
    "\n",
    "train_matrix = np.array(vectorizer.transform(train_x).todense())\n",
    "test_matrix =  np.array(vectorizer.transform(test_x).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"Running SVM\"\n",
    "lin_clf = LinearSVC(C=1.0)\n",
    "y_pred_linsvm = lin_clf.fit(train_matrix, train_y).predict(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_acc = np.mean(y_pred_linsvm==train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"SVM finihsed with\", train_acc,\"accuracy on train_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"Predicting on test set...\"\n",
    "test_preds = lin_clf.predict(test_matrix)\n",
    "\n",
    "for idx,label in test_y.items():\n",
    "    if  idx in already:\n",
    "        test_preds[idx] = test_y[idx]\n",
    "        continue\n",
    "    test_preds[idx] = test_preds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"writing predictions and model to the file\"\n",
    "with open(save_dir+\"test_labels.csv\",\"w\") as f:\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "    for i,l in zip(test_id,test_preds):\n",
    "        f.write(str(i)+\",\"+str(l)+\"\\n\")\n",
    "        \n",
    "with open(save_dir+\"model.pkl\",\"w\") as f:\n",
    "    pickle.dump(lin_clf,f)\n",
    "\n",
    "print \"FINISHED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#code to estimate c\n",
    "# def estimate_c():\n",
    "#     cval = []\n",
    "#     for exp in range(0,5,1):\n",
    "#         cval.append(2**exp)\n",
    "        \n",
    "#     best_score = 0\n",
    "#     best_c = None\n",
    "    \n",
    "#     for c in cval:\n",
    "#         lin_clf = LinearSVC(C=c)\n",
    "#         scores = cross_val_score(lin_clf, train_matrix, train_y, cv=10)\n",
    "#         score = np.mean(scores)\n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_c = c\n",
    "#         print score, c\n",
    "#     return best_c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
